# Partisan Hashtags

This repository contains all of the code needed to reproduce the analysis done in the blog post Partisan Hashtags.
Unfortunately it's against Twitter's terms of service to distribute tweets or derivatives of them, and it's my opinion the dataset, which is a csv of hashtags, screen names, and tweet times, falls under the latter category.
Good news for you, there are _two_ ways to run this notebook!
Hooray simplicity.

## Exact Reproduction

Requirements:

* Sunlight Foundation Python API (`pip install sunlight`)
* jq (via your favorite package manager)
* Seaborn (`pip install seaborn`)
* NetworkX (`pip install networkx`)

You can _exactly_ reproduce the dataset by "hydrating" the tweet IDs and extracting the hashtags you need with [jq](https://stedolan.github.io/jq/).
The program I used to scrape the tweets with the REST API, [tweetshovel](https://github.com/timothyrenner/tweetshovel), doesn't support this yet.
There's another tool called [twarc](https://github.com/edsu/twarc) that does.
You need a set of API keys to use it, and it could take some time (probably not a _huge_ amount of time), since Twitter throttles the number of tweets you can pull within certain time windows.
You can read more about Twitter's rate limits [here](https://dev.twitter.com/rest/public/rate-limiting).
The endpoint for hydration is [`/statuses/lookup`](https://dev.twitter.com/rest/reference/get/statuses/lookup).
From what I can tell (haven't used it myself), twarc returns the tweets as line-oriented JSON objects, which can be piped into the following jq program to get the data in precisely the same format:

```
jq --raw-output '[[.user.screen_name], [.created_at], [.id_str]
				  [.entities.hashtags | .[].text]] | combinations | 
				  join(\",\")' > hashtags.csv'
```

Obviously you'll need to put line extensions in there, and perhaps even pause a moment and admire just how freaking _awesome_ jq is for making is possible to build this csv without leaving the command line.
Save that csv file in a directory named `data/` as `congress_last_week_hashtags.py`, or point the notebook to whatever you ended up saving the data as.

The other thing you'll need is a csv of legislators.
Thankfully, I've got you covered there, but you'll need the Sunlight Foundation's Python module to run it:

```
python3.5 get_congress.py
```

This puts a file called `congress.csv` in a directory `data/`.

Then you're all set to run the notebook.

## Fresh Data

Requirements:

* [tweetshovel](https://github.com/timothyrenner/tweetshovel) - placed in `bin/` in the project root, also requires an API key JSON file, see link for details
* Sunlight Foundation Python API (`pip install sunlight`)
* jq (your favorite package manager)
* [Gephi](https://gephi.org/) - if you want to make the fancy PNGs with the new data
* pandas (`pip install pandas`)
* Seaborn (`pip install seaborn`)
* NetworkX (`pip install networkx`)

This is a little more complicated to run, so here's a step-by-step guide:

1. Run `python3.5 get_congress.py`. 
This will download the Congress data from the Sunlight API and put the relevant CSV in `data/congress.csv`.

2. Run `python3.5 get_congress_tweets.py`. 
This script looks at the data in `congress.csv` and runs tweetshovel and jq for _every_ twitter account in Congress.
Tweetshovel isn't wholly optimized for this ... we'll say ... experience, so it takes ***way*** longer than it should.
Like 12 hours.
I've got [plans](https://github.com/timothyrenner/tweetshovel/issues) for improving it based on my experiences with using it in this context.
This will create a directory called `hashtags/` and drop a csv in it for each member of Congress containing the hashtag data.

3. Run `python3.5 get_congress_hashtags.py`, which concatenates all of the hashtag files generated by the above step, truncates the dates to one week (which is ***hard coded - change it when pulling new data*** - #sorry), and does just a little munging on the dates and hashtags, dumping the results into `data/congress_last_week_tags.csv`.
It also creates a file `data/congress_tweet_ids.csv` which contains all of the tweet IDs in the dataset.
That way if you want to wait on Twitter's API again for some reason, you can.

4. Launch the notebook.
All of the files should be in place to run except fancy Gephi-powered PNGs, which will be for the original dataset I used when I wrote the blog post.
To get a new graph picture, run the notebook until it saves a `.gexf` file, open that file in Gephi, and go nuts.
I selected only the giant component and used a Force Atlas 2 layout to make the pictures, but the sky's the limit with Gephi until it crashes or decides to stop drawing stuff.
